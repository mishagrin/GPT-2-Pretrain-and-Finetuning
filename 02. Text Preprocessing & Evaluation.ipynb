{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20457554-4ed0-423c-868a-34e53dd52928",
   "metadata": {},
   "source": [
    "# Russian Poetry Generation\n",
    "\n",
    "#### GPT-2 Pretrain & Fine-Tuning from scratch\n",
    "\n",
    "## Setup\n",
    "\n",
    "#### Load Taiga Proza Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97318467-eacd-4968-a078-ea233a494263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "datasets_path = Path(r'datasets')\n",
    "\n",
    "df = pd.read_csv(datasets_path/'taiga_proza_1GB_43350.csv', sep='|', encoding='utf-8-sig')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5d2c0e-da92-4f40-a2a4-399e127d8fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty rows\n",
    "df.isna().sum()\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c472f8e5-c47e-44ee-be21-4936f988f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create a Huggingface Dataset Instance\n",
    "taiga_proza_russian = Dataset.from_pandas(df)\n",
    "taiga_proza_russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67dea76-f645-4040-bd35-b0c7b46d4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first sample\n",
    "taiga_proza_russian[0]['text'][:512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2420294b-a649-4e25-8ee1-7de2338a4b55",
   "metadata": {},
   "source": [
    "#### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d9a4d-6980-494f-a381-3c16d75e6d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import regex\n",
    "\n",
    "from functools import partial\n",
    "from num2words import num2words\n",
    "\n",
    "programming_quotes_pattern = r'\".+\"' # replace \"\" with «»\n",
    "fake_dash_pattern = r'-{2,}' # replace -- with —\n",
    "replace_number_pattern = r'[0-9]+' # replace number to word format\n",
    "cyrillic_only_pattern = r'[^#А-Яа-яёй\\-—\\.,:;!?…|«»„“\\s]' # leave only this letters\n",
    "broken_manydots_pattern = r'(\\.){2,}' # replace ... with …\n",
    "double_punct_pattern = r'[\\-—\\.,:;!?…]{2,}' # if two punctuations are in place, leave first\n",
    "double_space_pattern = r'\\s{2,}'\n",
    "garbage_punctuation_pattern = r'(?<=[А-Яа-я]\\s{1,})[\\-—\\.,:;!?…|«»„“\\s]{2,}(?=\\s+[А-Яа-я])' # replace — . — with first punct\n",
    "space_after_punct_pattern = r'[\\.,:;!?…](?![\\s«»„“])' # make sure all classic punctuations have space after them\n",
    "remove_space_before_punct_pattern = r'\\s+[\\.,:;!?…]' # make sure there is no space before punctuation\n",
    "m_dash_with_space_pattern = r'((?<!\\s)—)|(—(?!\\s))' # make sure m-dash with spaces around it\n",
    "#fix_hyphen_pattern = r'(?<=(?:бледно|почивала|не|давай|выпукло|толстый|норд|западно|день|умница|да|пару|когда|генерал|паинька|луна|опять|буль|один|где|хухры|кой|как|киловатт|что|много|который|давным|винни|англо|жар|мать|путь|она|елки|во|тихо|баба|бутылочно|так|крепко|чуть|волей|пол|гав|ей|кто|рад|нет|пила|изжелта|поди|премьер|юго|купля|прямо|тик|кошки|нежданно|северно|крест|черным|узнал|из|бой|ну|южно|восточно|запад|северо|маленький|в|изба|мини|еле|цып|страсти|кол|дизель|зюйд|динь|сегодня|человеко|статс|ням|по|никогда|мало|на|кое|ого|экс|штабс|север|для|пепельно|раз|какой|год|постояли|вице|завтра|фигли|темно|все|миру)(\\s+)?)\\s?[\\-—]\\s?(?=(\\s+)?(?:мотор|соловьиному|юбка|яблока|а|под|первых|день|своему|птичьи|тка|чем|черно|ка|единственный|русый|рыба|накрепко|буль|где|разумница|богу|мигли|иному|что|мир|палки|продажа|никак|вторых|много|капитан|русски|одинешенек|западный|нибудь|куда|час|моему|европейская|птица|во|дама|баба|восточный|так|го|тройку|восток|ледовитый|чуть|накрест|пух|неволей|гав|читальня|кто|мордасти|нет|яга|братски|видимому|розовый|вогнутый|деньской|пустому|либо|героиня|радехонек|мальски|сибирская|с|запад|мухры|маленький|третьих|дорога|то|мальчик|еле|назавтра|цып|таки|зеленый|мышки|лимона|губернатор|серый|динь|давно|ням|красный|сахалинск|прежнему|помалу|адмирал|какими|японский|другой|над|майор|негаданно|смирно|за|какой|немецки|два|постояли|завтра|ост|претолстый|министр|вест|парк|де|юг|гора))' # fix incorrect hyphen spaces around most common words\n",
    "fix_hyphen_pattern = r'(?<=(?<![А-Яа-яЙЁйё])(?:бледно|почивала|не|давай|выпукло|толстый|норд|западно|день|умница|да|пару|когда|генерал|паинька|луна|опять|буль|один|где|хухры|кой|как|киловатт|что|много|который|давным|винни|англо|жар|мать|путь|она|елки|во|тихо|баба|бутылочно|так|крепко|чуть|волей|пол|гав|ей|кто|рад|нет|пила|изжелта|поди|премьер|юго|купля|прямо|тик|кошки|нежданно|северно|крест|черным|узнал|из|бой|ну|южно|восточно|запад|северо|маленький|в|изба|мини|еле|цып|страсти|кол|дизель|зюйд|динь|сегодня|человеко|статс|ням|по|никогда|мало|на|кое|ого|экс|штабс|север|для|пепельно|раз|какой|год|постояли|вице|завтра|фигли|темно|все|миру|)(\\s+)?)\\s?[\\-—]\\s?(?=(\\s+)?(?:мотор|соловьиному|юбка|яблока|а|под|первых|день|своему|птичьи|тка|чем|черно|ка|единственный|русый|рыба|накрепко|буль|где|разумница|богу|мигли|иному|что|мир|палки|продажа|никак|вторых|много|капитан|русски|одинешенек|западный|нибудь|куда|час|моему|европейская|птица|во|дама|баба|восточный|так|го|тройку|восток|ледовитый|чуть|накрест|пух|неволей|гав|читальня|кто|мордасти|нет|яга|братски|видимому|розовый|вогнутый|деньской|пустому|либо|героиня|радехонек|мальски|сибирская|с|запад|мухры|маленький|третьих|дорога|то|мальчик|еле|назавтра|цып|таки|зеленый|мышки|лимона|губернатор|серый|динь|давно|ням|красный|сахалинск|прежнему|помалу|адмирал|какими|японский|другой|над|майор|негаданно|смирно|за|какой|немецки|два|постояли|завтра|ост|претолстый|министр|вест|парк|де|юг|гора)(?![А-Яа-яЙЁйё]))' # fix incorrect hyphen spaces around most common words\n",
    "replace_hyphen_with_dash_pattern = r'((?<=\\s)-)|(-(?=\\s))' # replace hyphen with mdash where it required\n",
    "replace_hyphen_with_dash_second_step_pattern = r'(\\s[\\-])|(\\s[\\-])' # where hyphen is sticked to word it's a dash\n",
    "\n",
    "programming_quotes = partial(re.sub, programming_quotes_pattern, lambda m: f'«{m.group(0)[1:-1].strip()}»')\n",
    "fake_dash = partial(re.sub, fake_dash_pattern, '—')\n",
    "replace_number = partial(re.sub, replace_number_pattern, lambda m: num2words(int(m.group(0)), lang='ru'))\n",
    "cyrillic_only = partial(re.sub, cyrillic_only_pattern, '')\n",
    "broken_manydots = partial(re.sub, broken_manydots_pattern, '…')\n",
    "double_punct = partial(re.sub, double_punct_pattern, lambda m: m.group(0)[0], flags=re.MULTILINE)\n",
    "remove_double_space = partial(re.sub, double_space_pattern, ' ')\n",
    "garbage_punctuation = partial(regex.sub, garbage_punctuation_pattern, lambda m: m.group(0)[0])\n",
    "space_after_punct = partial(re.sub, space_after_punct_pattern, lambda m: f'{m.group(0)} ')\n",
    "remove_space_before_punct = partial(re.sub, remove_space_before_punct_pattern, lambda m: f'{m.group(0).strip()}', flags=re.MULTILINE)\n",
    "m_dash_with_space = partial(re.sub, m_dash_with_space_pattern, lambda m: f' {m.group(0)} ')\n",
    "fix_hyphen = partial(regex.sub, fix_hyphen_pattern, '-', flags=re.IGNORECASE)\n",
    "replace_hyphen_with_dash = partial(re.sub, replace_hyphen_with_dash_pattern, '—', flags=re.MULTILINE)\n",
    "replace_hyphen_with_dash_second_step = partial(re.sub, replace_hyphen_with_dash_second_step_pattern, ' — ', flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebed123-8bc9-4370-85aa-6577e8633c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_taiga_sample(samples):\n",
    "    text_batch = []\n",
    "\n",
    "    for sample in samples[\"text\"]:\n",
    "\n",
    "        sample = programming_quotes(sample)\n",
    "        sample = fake_dash(sample)\n",
    "        sample = broken_manydots(sample)\n",
    "        #sample = replace_number(sample) # ignore, for now\n",
    "        sample = cyrillic_only(sample)\n",
    "        sample = m_dash_with_space(sample)\n",
    "        sample = fix_hyphen(sample)\n",
    "        sample = replace_hyphen_with_dash(sample)\n",
    "        sample = remove_double_space(sample)\n",
    "        sample = space_after_punct(sample)\n",
    "        sample = replace_hyphen_with_dash_second_step(sample)\n",
    "        sample = garbage_punctuation(sample)\n",
    "        sample = remove_space_before_punct(sample)\n",
    "        sample = double_punct(sample)\n",
    "        \n",
    "        sample = sample.strip()\n",
    "\n",
    "        if sample != '':\n",
    "            sample = sample[0].capitalize() + sample[1:]\n",
    "        \n",
    "        text_batch += [sample]\n",
    "\n",
    "    return {\n",
    "        \"text_cleaned\": text_batch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da65274-242b-4c81-a517-7ce052466eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "taiga_proza_russian = taiga_proza_russian.map(\n",
    "    cleanup_taiga_sample, \n",
    "    batched=True,\n",
    "    remove_columns=taiga_proza_russian.column_names\n",
    ")\n",
    "taiga_proza_russia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf2efeb-8d5f-45e4-9621-161f85851d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Save & load to disk. Will save up some RAM\n",
    "taiga_proza_russian.save_to_disk(str(datasets_path/'taiga-corpus-full-russian-cleaned'))\n",
    "taiga_proza_russian = load_from_disk(str(datasets_path/'taiga-corpus-full-russian-cleaned'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0dea9c-1348-4f75-b16c-e0564b2f87b6",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "#### Tokenization & arabization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67852776-a91c-4740-a043-d8a3703febb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "NEWLINE_TAG = '<|newline|>' # later, will break poem to separate lines\n",
    "\n",
    "def sentence_and_word_tokenization(sample: str) -> list[list[str]]:\n",
    "    # 01. Split text into nested list, with sentences and words\n",
    "    sentence_tokens = [word_tokenize(t) for t in sent_tokenize(sample)]\n",
    "\n",
    "    # 02. Cleanup non-cyrillic symbols\n",
    "    filtered_list = [[item.strip() for item in sublist if item.strip()] for sublist in [[item for item in sublist] for sublist in sentence_tokens]]\n",
    "\n",
    "    # 03. Remove empty elements\n",
    "    filtered_list = [i for i in filtered_list if i]\n",
    "    return filtered_list\n",
    "\n",
    "def arabize_nested_list(nested_list: list[list[str]]) -> list[list[str]]:\n",
    "    return [[item for item in reversed(inner_list)] for inner_list in nested_list]\n",
    "\n",
    "def apply_nl_tag(nested_list: list[list[str]]) -> list[list[str]]:\n",
    "    result_list: list[list[str]] = []\n",
    "\n",
    "    for i, inner_list in enumerate(nested_list):\n",
    "        result_list.append(inner_list)\n",
    "        result_list.append([NEWLINE_TAG])\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943cdd51-abda-4333-9e1d-b40e2327c6c5",
   "metadata": {},
   "source": [
    "Get original sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0328d4-c04f-486b-bf5e-13f9481d09b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = taiga_proza_russian[0]['text_cleaned']; sample[:512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd1ca9b-5eeb-4535-9e90-6ab046a75274",
   "metadata": {},
   "source": [
    "Split it into sentences and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a277c76-da0e-4144-8c5d-97814cc57180",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_and_word_tokenization(sample)[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a7fe9d-8836-4010-bf51-0c3003b5d4b9",
   "metadata": {},
   "source": [
    "Arabize words in each sentence. Keep order of sentences, but reverse words order in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd388de6-2c19-4b96-8f81-cd2be34a2ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabize_nested_list(sentence_and_word_tokenization(sample))[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac7f1e-24ce-48ad-829d-4472ec78ac64",
   "metadata": {},
   "source": [
    "Apply special tag after each sentence. Which will indicate end of the sentence.\n",
    "\n",
    "```python\n",
    "'<|newline|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b3d8f4-3138-43c9-9123-4c2e570a8cf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apply_nl_tag(arabize_nested_list(sentence_and_word_tokenization(sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ee53e0-8c25-4712-8933-10347dadce3a",
   "metadata": {},
   "source": [
    "#### Split into syllables & add stresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bfb041-e757-4980-87f3-86ff9aba1425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from rusyllab import rusyllab\n",
    "from stressrnn import StressRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4848967f-1583-478d-8651-2d282458bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_rnn = StressRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32091164-5208-4ef5-9827-a41718024fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_sum(numbers):\n",
    "    return list(itertools.accumulate(numbers))\n",
    "\n",
    "# Lookup table of regular cyrillic vowels with stressed variants\n",
    "vowels_mappings = {\n",
    "\"А'\": \"А́\", \"Е'\": \"Е́\", \"Ё'\": \"Ё́\", \"И'\": \"И́\", \"Й'\": \"Й́\", \"О'\": \"О́\", \"У'\": \"У́\", \"Ы'\": \"Ы́\", \"Э'\": \"Э́\", \"Ю'\": \"Ю́\", \"Я'\": \"Я́\",\n",
    "\"а'\": \"а́\", \"е'\": \"е́\", \"ё'\": \"ё́\", \"и'\": \"и́\", \"й'\": \"й́\", \"о'\": \"о́\", \"у'\": \"у́\", \"ы'\": \"ы́\", \"э'\": \"э́\", \"ю'\": \"ю́\", \"я'\": \"я́\",   \n",
    "\"А\": \"А́\", \"Е\": \"Е́\", \"Ё\": \"Ё́\", \"И\": \"И́\", \"Й\": \"Й́\", \"О\": \"О́\", \"У\": \"У́\", \"Ы\": \"Ы́\", \"Э\": \"Э́\", \"Ю\": \"Ю́\", \"Я\": \"Я́\",   \n",
    "\"а\": \"а́\", \"е\": \"е́\", \"ё\": \"ё́\", \"и\": \"и́\", \"й\": \"й́\", \"о\": \"о́\", \"у\": \"у́\", \"ы\": \"ы́\", \"э\": \"э́\", \"ю\": \"ю́\", \"я\": \"я́\",\n",
    "}\n",
    "\n",
    "vowels = \"аеёийоуыэюя\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc95c44-1515-4126-a4e1-c0ce37dc4b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_syllables_add_stresses (tokens: list[str]) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Preprocess tokens into syllables with stresses. ['Парадокс'] -> ['Па', 'ра', 'до́кс'].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens : list\n",
    "        list of tokens, where each token is a single word. For example, ['Глава', 'первая', 'Михаил', 'Иванович', ...]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "        return string, where each word separated by | symbol.\n",
    "    \"\"\"\n",
    "\n",
    "    # 01. We will return preprocessed proza as nested list\n",
    "    preprocessed_tokens: list[list[str]] = []\n",
    "\n",
    "    # 02. Iterate each word in sentence\n",
    "    for token in tokens:\n",
    "\n",
    "        # IMPORTANT\n",
    "        # Clean empty spaces\n",
    "        token = token.strip().replace(' ', '')\n",
    "\n",
    "        # 03. Keep track of:\n",
    "        # - Each vowel in the word\n",
    "        # - Index of stressed vowel\n",
    "        # - Stressed character\n",
    "        vowel_indexes = {} # {1: 'е', 4: 'а', 5: 'я'}\n",
    "        stress_vowel_index = None # 1\n",
    "        stress_vowel_char = None # 'e'\n",
    "\n",
    "        # 02. Check each character in token\n",
    "        #     If it's a vowel store it and get it's index\n",
    "        for i, char in enumerate(token):\n",
    "            if char.lower() in vowels:\n",
    "                vowel_indexes[i] = char\n",
    "\n",
    "        # 03. Find stress in token\n",
    "        #     Result will be same token but with ' char after vowel\n",
    "        #     \"Глава\" -> \"Глава'\" -- stress ' added after last а\n",
    "        stressed_token = stress_rnn.put_stress(\n",
    "            token,\n",
    "            stress_symbol=\"'\",\n",
    "            accuracy_threshold=0.5,\n",
    "            replace_similar_symbols=True\n",
    "        )\n",
    "\n",
    "        # If stress is not found, just add word as it is\n",
    "        if \"'\" not in stressed_token:\n",
    "            syllables: list[str] = rusyllab.split_word(token)\n",
    "            preprocessed_tokens.append(syllables)\n",
    "            continue\n",
    "\n",
    "        # 04. Find index of stressed vowel\n",
    "        #     Store it in variables\n",
    "        #     Potentially error\n",
    "        for k, v in vowel_indexes.items():\n",
    "            if stressed_token[k + 1] == \"'\":\n",
    "                stress_vowel_index = k\n",
    "                stress_vowel_char = v\n",
    "\n",
    "        # If stress_vowel_index is None, just add as it is\n",
    "        if stress_vowel_index == None:\n",
    "            syllables: list[str] = rusyllab.split_word(token)\n",
    "            preprocessed_tokens.append(syllables)\n",
    "            continue\n",
    "\n",
    "        # 05. Break word into syllables\n",
    "        syllables: list[str] = rusyllab.split_word(token) # 'Парадокс' -> ['Па', 'ра', 'докс']\n",
    "\n",
    "        # 06. Count len of each syllable part\n",
    "        #     ['Па', 'ра', 'докс'] -> [len('Па'), len('ра'), len('докс')] -> [2, 2, 4]\n",
    "        syllables_lens: list[int] = [len(s) for s in syllables]\n",
    "        \n",
    "        # 06. Count cumulative sum of each syllable lens\n",
    "        #     If [i, k, j] it's a len of a syllable, then [i, i + k, i + k + j]\n",
    "        sum_syllables_lens: list[int] = cumulative_sum(syllables_lens)\n",
    "\n",
    "        # 07. Store stressed syllables here\n",
    "        #     We will find what syllable to stress next\n",
    "        stressed_syllables: list[str] = []\n",
    "\n",
    "        # Check if stress char is already replaced\n",
    "        char_is_replased = False\n",
    "\n",
    "        # 08. Replace regular vowel in splitted word with stress vowel\n",
    "        #     ['Па', 'ра', 'докс'] -> ['Па', 'ра', 'до́кс']\n",
    "        for i, (s, sum) in enumerate(zip(syllables, sum_syllables_lens)):\n",
    "\n",
    "            # 08.1. Add syllable to the array\n",
    "            stressed_syllables.append(s)\n",
    "\n",
    "            # 08.2. If index of stressed vowel is bigger than cumulative sum of syllable\n",
    "            #       Then continue to the next loop iteration. The stressed vowel is not here\n",
    "            if stress_vowel_index + 1 > sum:\n",
    "                continue\n",
    "            else:\n",
    "                if not char_is_replased:\n",
    "                    # 08.3. Overwise replace regular vowel with stressed one\n",
    "                    ss = s.replace(stress_vowel_char, vowels_mappings[stress_vowel_char])\n",
    "    \n",
    "                    # 08.4. Update syllable in array\n",
    "                    stressed_syllables[i] = ss\n",
    "    \n",
    "                    char_is_replased = True\n",
    "\n",
    "        # Reverse syllables ['Па', 'ра', 'до́кс'] -> ['до́кс', 'ра', 'Па']\n",
    "        reversed_stressed_syllables = stressed_syllables[::-1]\n",
    "        \n",
    "        # Add stressed syllables to the array ['до́кс', 'ра', 'Па']\n",
    "        preprocessed_tokens.append(reversed_stressed_syllables)\n",
    "\n",
    "    return preprocessed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352bb690-55c4-4710-b9d5-87e9a8d78915",
   "metadata": {},
   "source": [
    "Add stresses to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbf6bc8-b68a-4c8d-aeba-1f879ac63ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_into_syllables_add_stresses(apply_nl_tag(arabize_nested_list(sentence_and_word_tokenization(sample)))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6db601-e226-4e6b-8d36-ac63772eb9d7",
   "metadata": {},
   "source": [
    "#### Tokenize & arabize whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e006cb0-2220-4649-96eb-8c1cd6b0df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sample(sample):\n",
    "    tokens = sentence_and_word_tokenization(sample['text_cleaned'])\n",
    "    tokens = arabize_nested_list(tokens)\n",
    "    tokens = apply_nl_tag(tokens)\n",
    "    return {\"tokens\": tokens}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dfa293-4a47-438e-baf6-dac0394df934",
   "metadata": {},
   "source": [
    "Apply to whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3463f8-f321-4fe6-abd9-d1ca12dc962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "taiga_proza_preprocessed = taiga_proza_russian.map(\n",
    "    preprocess_sample, batched=False\n",
    ")\n",
    "\n",
    "taiga_proza_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee69741-6b2e-4ae6-8647-e63e38c3d93e",
   "metadata": {},
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd8ca1-c7c2-474d-b144-01662c870954",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "taiga_proza_preprocessed[0]['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498b313-a466-478c-8009-8dbfa06ae03b",
   "metadata": {},
   "source": [
    "Save & load back from. This will reduce RAM usage on later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0541a9d-dc90-4d56-805a-165602928289",
   "metadata": {},
   "outputs": [],
   "source": [
    "taiga_proza_preprocessed.save_to_disk(str(datasets_path/'taiga-corpus-full-russian-cleaned-preprocessed'))\n",
    "taiga_proza_preprocessed = load_from_disk(str(datasets_path/'taiga-corpus-full-russian-cleaned-quarter-preprocessed'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ac6c85-e098-44a4-8130-9c8e06cf86c0",
   "metadata": {},
   "source": [
    "#### Break tokens into syllables & add stresses to whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ae169-752a-43bf-a7c1-7361ac1510f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllabize_and_stress_words (tokens: list[str]) -> str:\n",
    "    # 01. Split each token into syllables, add stresses\n",
    "    #     ['Парадокс'] -> ['до́кс', 'ра', 'Па']\n",
    "    syllabized = split_into_syllables_add_stresses(tokens)\n",
    "\n",
    "    # 02. Join each word with | symbol\n",
    "    joined_sentence = ' | '.join([' '.join(syls) for syls in syllabized])\n",
    "\n",
    "    return joined_sentence\n",
    "\n",
    "def syllabize_and_stress_sample(sample):\n",
    "    syllables = ' '.join([syllabize_and_stress_words(tokens) if tokens != [NEWLINE_TAG] else ''.join(tokens) for tokens in sample['tokens']])\n",
    "    return {\"syllables\": syllables}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f514a9b5-f9b2-4fbc-9c30-80479f6eb55a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "syllabize_and_stress_sample(taiga_proza_preprocessed[0])['syllables']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b557db8-1b3c-42c7-9842-aa1cd532faad",
   "metadata": {},
   "source": [
    "Preprocess whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e7e117-f734-47fa-b37f-f3a8a84c8f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "taiga_proza_stressed = taiga_proza_preprocessed.map(\n",
    "    syllabize_and_stress_sample, batched=False\n",
    ")\n",
    "taiga_proza_stressed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bffa4-df25-4c8d-b785-8f6f0c19f90d",
   "metadata": {},
   "source": [
    "Add quatrain tag after each 4th newline tag. This will indicates a quatrain. Later it will help model to write poetry with quatrains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1b3412-5ce1-407c-92b5-09067cef1de2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from functools import partial\n",
    "\n",
    "QUATRAIN_TAG = '<|quatrain|>'\n",
    "\n",
    "apply_quatrain_tag = partial(re.sub, '(<\\|newline\\|>)', lambda m, c = itertools.count(start=1): m.group() if next(c) % 4 else NEWLINE_TAG + QUATRAIN_TAG)\n",
    "\n",
    "apply_quatrain_tag(taiga_proza_stressed[0]['syllables'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce1b241-737d-4708-be02-8be8a876238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_quatrain_sample(sample):\n",
    "    return {\"quatrained\": apply_quatrain_tag(sample['syllables'])}\n",
    "\n",
    "taiga_proza_stressed = taiga_proza_stressed.map(\n",
    "    apply_quatrain_sample, batched=False\n",
    ")\n",
    "taiga_proza_stressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3fd3a3-6777-4956-8cb6-fdedaf3225f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "taiga_proza_stressed.save_to_disk(str(datasets_path/'taiga-corpus-full-russian-cleaned-quarter-preprocessed-stressed'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4bbace-a52e-4862-bcb9-eb0c774343ae",
   "metadata": {},
   "source": [
    "## Create Tokenizer From Scratch\n",
    "\n",
    "https://huggingface.co/learn/nlp-course/chapter6/8?fw=pt\n",
    "\n",
    "[add_special_tokens.example. Add special tokens to already pretrained model or tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.add_special_tokens.example)\n",
    "\n",
    "#### Create BPE Tokenizer\n",
    "\n",
    "GPT-2 compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9febedc0-4571-4bcd-9256-35501e0be5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcb5ec0-98cf-4298-a2c9-dd4d8bf3d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c4cb5d-1c4b-4800-939b-075bec1ddc73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(taiga_proza_stressed[0]['quatrained'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c1a59-6c82-49ac-ae1e-aa38a7713828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus(num_batches = 10):\n",
    "    return (\n",
    "        taiga_proza_stressed[i : i + num_batches][\"quatrained\"]\n",
    "        for i in range(0, len(taiga_proza_stressed), num_batches)\n",
    "    )\n",
    "\n",
    "training_corpus = get_training_corpus()\n",
    "training_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a55323-e74b-4c38-a14f-9c2ef3054ca7",
   "metadata": {},
   "source": [
    "#### Setup BPE Tokenizer trainer\n",
    "\n",
    "https://huggingface.co/docs/tokenizers/api/trainers#tokenizers.trainers.BpeTrainer\n",
    "\n",
    "Include stressed and regular vowels as special tokens. To treat them as single letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0008ebb3-63b0-4f33-9a73-b2ad5846fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=52000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\n",
    "        \"<|endoftext|>\",\n",
    "        \"А́\", \"Е́\", \"Ё́\", \"И́\", \"Й́\", \"О́\", \"У́\", \"Ы́\", \"Э́\", \"Ю́\", \"Я́\",\n",
    "        \"а́\", \"е́\", \"ё́\", \"и́\", \"й́\", \"о́\", \"у́\", \"ы́\", \"э́\", \"ю́\", \"я́\",\n",
    "        \"А\", \"Е\", \"Ё\", \"И\", \"Й\", \"О\", \"У\", \"Ы\", \"Э\", \"Ю\", \"Я\",\n",
    "        \"а\", \"е\", \"ё\", \"и\", \"й\", \"о\", \"у\", \"ы\", \"э\", \"ю\", \"я\",\n",
    "        NEWLINE_TAG,\n",
    "        QUATRAIN_TAG,\n",
    "        \"-\", \"—\", \".\", \",\", \":\", \";\", \"!\", \"?\", \"…\", \"|\",\n",
    "        #\"«\", \"»\", # not working as expected\n",
    "        \"„\", \"“\"\n",
    "    ],\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1d4c6d-b411-4c26-93ac-0e50afd97c21",
   "metadata": {},
   "source": [
    "Train BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2dadb5-df15-412d-9f3f-178a65e4a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(training_corpus, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a744fe82-dcd4-4b51-ae9c-85bfaa7f4420",
   "metadata": {},
   "source": [
    "See results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ac8c9-efdd-4c60-bc0b-7b459324648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(taiga_proza_stressed[0]['quatrained'][:512])\n",
    "\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c180b0a-687c-4262-b6a3-8ebe223515df",
   "metadata": {},
   "source": [
    "Init ByteLevel decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe1aea7-7379-4732-b7c4-e913146c422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.decoders import ByteLevel\n",
    "\n",
    "decoder = ByteLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf0baf-ae4b-4b61-b3b3-370903ac5d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder.decode(tokenizer.encode(taiga_proza_stressed[0]['quatrained'][:512]).tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11ccae4-d4fd-41cf-9b54-60df69959283",
   "metadata": {},
   "source": [
    "Check special stressed characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69cf51a-03a8-4774-ad5d-6fd884aaaec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode(\"а́\").tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a7b1fb-ecd6-486e-986c-1d7b722835f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder.decode(['а́']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aecb42-c8bc-4f4b-ae84-8e6210c5fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder.decode(['ĠÐĵÐ»']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ac6e1a-3aae-4773-b903-131a4bc54934",
   "metadata": {},
   "source": [
    "Apply byte-level postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1492a2de-4471-4b70-876c-ef41d689176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba984b-c67f-411b-b449-c43d1ef8dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf9d2c3-a08e-41e0-96cc-5843b8e399ae",
   "metadata": {},
   "source": [
    "Check how it works. Show special tokens, because they are important part of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601863e-dbaf-4809-b0d8-32bdd34b6711",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(taiga_proza_stressed[0]['quatrained'][:512]).ids, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9e7955-66f9-446a-ac42-fadfa0fa01e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('« »').ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d20f9db-9e8d-4276-bc48-cac339052298",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([102, 82, 357], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7039c831-8bca-404a-9669-3143d5f3df17",
   "metadata": {},
   "source": [
    "Wrap pretrained tokenizer into GPT2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5433ae2b-802e-47b2-88c2-caa22c7a2688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)\n",
    "wrapped_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f566a61c-071f-44a9-8fb5-af3f28cb2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer.save_pretrained(\"tokenizers/poetry-generator-punctuation-quatrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fda7b66-459c-455b-8f17-1b9db8bae203",
   "metadata": {},
   "source": [
    "## Train/valid/test splits\n",
    "\n",
    "#### Split the data, remove unnecessary colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6725ddaf-ea48-4412-bd92-67012d88f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Setup split seed for reproducibility\n",
    "SPLIT_SEED=42\n",
    "\n",
    "# Remove colums, rename syllables to text\n",
    "taiga_proza_stressed = taiga_proza_stressed.remove_columns(['text_cleaned', 'tokens', 'syllables'])\n",
    "#taiga_proza_stressed = taiga_proza_stressed.rename_column(\"text\", \"raw\")\n",
    "taiga_proza_stressed = taiga_proza_stressed.rename_column(\"quatrained\", \"text\")\n",
    "\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": taiga_proza_stressed\n",
    "})\n",
    "\n",
    "# 02. Create train/test split\n",
    "train_test_dataset = raw_datasets['train'].train_test_split(test_size=0.2, seed=SPLIT_SEED)\n",
    "\n",
    "# 03. Create test/validation split\n",
    "test_valid_dataset = train_test_dataset['test'].train_test_split(test_size=0.5, seed=SPLIT_SEED)\n",
    "\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_test_dataset['train'],\n",
    "    \"test\": test_valid_dataset['test'],\n",
    "    \"valid\": test_valid_dataset['train'],\n",
    "})\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6286bf0-6bf4-49f9-9af0-c0372d532e63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_datasets['train'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9ef345-fdc9-4169-9dbb-f4a0a9bfa278",
   "metadata": {},
   "source": [
    "Save & load back from disk. To reduce RAM usage later, when tokenization will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165a113e-f727-4f79-a1d0-2f83a7afc6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets.save_to_disk(str(datasets_path/'taiga-corpus-full-russian-cleaned-quarter-preprocessed-stressed_train_test_valid'))\n",
    "raw_datasets = load_from_disk(str(datasets_path/'taiga-corpus-full-russian-cleaned-quarter-preprocessed-stressed_train_test_valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea69cc7-793b-4288-ba20-8165170f2571",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "#### Load pretrained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc40cfb-a21e-442b-a53c-fc66c39cffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "context_length = 512 # we will train it on 4090 RTX\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizers/poetry-generator-punctuation-quatrained\")\n",
    "\n",
    "outputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:2][\"text\"],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d688a-aa75-43e8-a5e8-7039dafcc4b0",
   "metadata": {},
   "source": [
    "#### Tokenize whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef87f2-00c3-4334-b152-dad1444bb99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89050942-4598-489d-ad7c-635f55ede9ab",
   "metadata": {},
   "source": [
    "#### Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db412c21-c390-4246-ac2b-3a19f615057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.save_to_disk(str(datasets_path/'tokenized/taiga-corpus-full-russian-cleaned-quarter-preprocessed-stressed_train_test_valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8623f99e-fedc-41dc-ae2e-38be302cf6b2",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "## Pretrain GPT-2 on Preprocessed Taiga Proza\n",
    "\n",
    "#### See pretraining script\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276d3d46-a4c9-4132-8d54-cc5364d7c253",
   "metadata": {},
   "source": [
    "## Metrics. Perplexity\n",
    "\n",
    "https://huggingface.co/docs/transformers/perplexity#example-calculating-perplexity-with-gpt-2-in--transformers\n",
    "\n",
    "#### Do evaluation\n",
    "\n",
    "Init model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5c8ad-b152-4870-9ee4-2e8fce63e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer, GenerationConfig\n",
    "\n",
    "# Put here correct paths to the models and tokenizer of course\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizers/poetry-generator-lemmatized-arabized-syllabized-stressed-ver2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"models/taiga_proza_5GB_216750_syllables_stresses_arabized_ver2\")\n",
    "\n",
    "model.to(device) # important!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e1c247-ae77-4cb3-ae60-ec0a1a39494a",
   "metadata": {},
   "source": [
    "Get tensors. Take 500 samples, for faster inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2903fcd8-794e-4630-8418-bd1be22eb57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(' '.join(raw_datasets['test'][:500][\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafffa42-e1a1-4acf-87d3-fabd898d6973",
   "metadata": {},
   "source": [
    "Compute perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c6288-2dbe-412b-b3b2-0139a67040b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.n_positions\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())\n",
    "ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8329a1-f26d-46ac-bece-9054d130d202",
   "metadata": {},
   "source": [
    "## Fine-Tune GPT-2 on Russian Poetry Corpus\n",
    "\n",
    "#### See corresponding notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
